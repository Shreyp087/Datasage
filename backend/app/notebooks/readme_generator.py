from __future__ import annotations

from datetime import datetime, timezone
from typing import Any

from app.models.models import Dataset


class ReadmeGenerator:
    """
    Generates a structured README.md for any dataset.
    For AIID datasets, includes AIID-specific sections.
    """

    def generate(
        self,
        dataset: Dataset,
        eda_json: dict[str, Any],
        agent_reports: list[Any],
        notebook_results: dict[str, Any] | None = None,
    ) -> str:
        domain = dataset.domain.value if hasattr(dataset.domain, "value") else str(dataset.domain)
        quality_score = float((eda_json or {}).get("dataset_quality_score", 0) or 0)
        shape = (eda_json or {}).get("shape", {}) or {}
        columns = (eda_json or {}).get("columns", []) or []
        warnings = (eda_json or {}).get("warnings", []) or []

        quality_report: dict[str, Any] = {}
        for report in agent_reports:
            if str(getattr(report, "agent_name", "")).strip().lower() == "quality inspector":
                payload = getattr(report, "structured_json", None)
                if isinstance(payload, dict):
                    quality_report = payload
                break

        file_size_mb = float(dataset.file_size_bytes or 0) / (1024 * 1024)

        readme = f"""# {dataset.name}

> Auto-generated by DataSage Â· {datetime.now(timezone.utc).strftime('%B %d, %Y')}

---

## ğŸ“‹ Overview

| Field | Value |
|-------|-------|
| **Dataset Name** | {dataset.name} |
| **Domain** | {domain} |
| **Source** | {self._get_source(dataset)} |
| **Rows** | {int(shape.get('rows', 0) or 0):,} |
| **Columns** | {int(shape.get('cols', 0) or 0)} |
| **File Size** | {file_size_mb:.1f} MB |
| **Last Updated** | {dataset.uploaded_at.strftime('%Y-%m-%d') if dataset.uploaded_at else 'Unknown'} |
| **Quality Score** | {self._quality_badge(quality_score)} |

{self._get_description(dataset)}

---

## âš¡ Quick Start
```python
import pandas as pd

# Load the cleaned dataset
df = pd.read_csv('{dataset.name.lower().replace(' ', '_')}_clean.csv')

print(f"Shape: {{df.shape}}")
print(df.head())
```

---

## ğŸ“Š Dataset Summary

**{int(shape.get('rows', 0) or 0):,} records** across **{int(shape.get('cols', 0) or 0)} fields**

{self._column_table(columns)}

---

## âš ï¸ Data Quality

**Quality Score: {self._quality_badge(quality_score)}**

{self._quality_section(quality_report, warnings)}

---

{self._domain_section(dataset, eda_json, notebook_results)}

## ğŸ”¬ Assumptions & Limitations

{self._assumptions_section(dataset, eda_json)}

---

## ğŸ“– Citation

{self._citation_section(dataset)}

---

## ğŸ” Reproducing This Analysis

1. Download this dataset from the source listed above
2. Upload to [DataSage](https://datasage.io)
3. Select domain: **{domain}**
4. Run the pre-built notebook template for this domain
5. This README will be regenerated with updated findings

---

*Generated by DataSage Â· Responsible AI Research Platform*
*Analysis powered by AI agents â€” review all findings before publication*
"""
        return readme

    def _quality_badge(self, score: float) -> str:
        if score >= 91:
            return f"âœ¨ {score:.0f}/100 Excellent"
        if score >= 71:
            return f"ğŸŸ¢ {score:.0f}/100 Good"
        if score >= 41:
            return f"ğŸŸ¡ {score:.0f}/100 Fair"
        return f"ğŸ”´ {score:.0f}/100 Poor"

    def _column_table(self, columns: list[dict[str, Any]]) -> str:
        if not columns:
            return "_No column metadata available._"

        lines = [
            "| Column | Type | Role | Missing | Notes |",
            "|--------|------|------|---------|-------|",
        ]
        for col in columns:
            null_pct = float(col.get("null_pct", 0) or 0) * 100
            if null_pct > 30:
                null_str = f"â›” {null_pct:.0f}%"
            elif null_pct > 5:
                null_str = f"âš ï¸ {null_pct:.0f}%"
            else:
                null_str = f"âœ… {null_pct:.0f}%"

            note = ""
            outlier_count = int(col.get("outlier_count", 0) or 0)
            if outlier_count > 0:
                note = f"ğŸ”´ {outlier_count} outliers"

            lines.append(
                f"| `{col.get('name', '')}` | {col.get('dtype', '')} | "
                f"{col.get('role', '')} | {null_str} | {note} |"
            )
        return "\n".join(lines)

    def _quality_section(self, quality_report: dict[str, Any], warnings: list[Any]) -> str:
        lines: list[str] = []
        critical = quality_report.get("critical_issues", []) if isinstance(quality_report, dict) else []
        if critical:
            lines.append("### ğŸš¨ Critical Issues\n")
            for issue in critical:
                if isinstance(issue, dict):
                    lines.append(
                        f"- **`{issue.get('column', '')}`**: "
                        f"{issue.get('issue', '')} â€” *Fix: {issue.get('fix', '')}*"
                    )
                else:
                    lines.append(f"- {issue}")

        if warnings:
            lines.append("\n### âš ï¸ Warnings\n")
            for warning in warnings:
                lines.append(f"- {warning}")

        if not critical and not warnings:
            lines.append("âœ… No critical quality issues detected.")
        return "\n".join(lines)

    def _domain_section(
        self,
        dataset: Dataset,
        eda_json: dict[str, Any],
        notebook_results: dict[str, Any] | None,
    ) -> str:
        domain = dataset.domain.value if hasattr(dataset.domain, "value") else str(dataset.domain)
        if domain == "ai_incidents":
            return self._aiid_section(dataset, eda_json, notebook_results)
        if domain == "legal_litigation":
            return self._dail_section(dataset, eda_json)
        return ""

    def _aiid_section(
        self,
        dataset: Dataset,
        eda_json: dict[str, Any],
        notebook_results: dict[str, Any] | None,
    ) -> str:
        del eda_json
        schema = dataset.schema_json if isinstance(dataset.schema_json, dict) else {}
        section = "## ğŸ¤– AI Incident Database â€” Specific Notes\n\n"
        section += f"**Snapshot Date:** {schema.get('snapshot_date', 'Unknown')}\n"
        section += f"**Snapshot URL:** {schema.get('snapshot_url', 'Unknown')}\n\n"
        section += (
            "**Source:** [AI Incident Database](https://incidentdatabase.ai)\n\n"
            "**Original Citation:**\n"
            "> McGregor, S. (2021) Preventing Repeated Real World AI Failures "
            "by Cataloging Incidents: The AI Incident Database. IAAI-21.\n\n"
        )
        section += """### âš ï¸ AIID-Specific Limitations

1. **Reporting Bias** â€” Only incidents with media coverage are included.
   Harms affecting marginalized communities with less media presence
   are systematically underrepresented.

2. **Taxonomy Coverage** â€” CSET classifications are applied to a subset
   of incidents. Unclassified incidents should not be interpreted as
   having no harm type.

3. **Temporal Lag** â€” Recent incidents take time to be submitted and
   accepted. The most recent 6-12 months are likely undercounted.

4. **Entity Attribution** â€” Deployer/developer attribution is based on
   media reports, not legal determinations or internal records.

5. **Snapshot Stability** â€” Older incidents may be retroactively
   reclassified. Cross-snapshot comparisons require alignment checks.

"""
        if notebook_results:
            section += "### ğŸ“Š Key Findings From This Snapshot\n\n"
            summary_result = ((notebook_results or {}).get("cell_002", {}) or {}).get("result", {})
            if isinstance(summary_result, dict) and summary_result:
                total_incidents = summary_result.get("total_incidents")
                total_text = f"{int(total_incidents):,}" if isinstance(total_incidents, (int, float)) else "N/A"
                section += f"- **Total Incidents:** {total_text}\n"
                dr = summary_result.get("date_range", {}) if isinstance(summary_result.get("date_range"), dict) else {}
                section += f"- **Date Range:** {dr.get('earliest', 'Unknown')} to {dr.get('latest', 'Unknown')}\n"
                section += f"- **Top Harm Type:** {summary_result.get('top_harm_type', 'N/A')}\n"
                section += f"- **Top Sector:** {summary_result.get('top_sector', 'N/A')}\n"
        return section

    def _dail_section(self, dataset: Dataset, eda_json: dict[str, Any]) -> str:
        del dataset, eda_json
        return "## âš–ï¸ Legal Litigation Notes\n\nDomain-specific notes are not yet configured for this dataset domain.\n\n"

    def _citation_section(self, dataset: Dataset) -> str:
        schema = dataset.schema_json if isinstance(dataset.schema_json, dict) else {}
        domain = dataset.domain.value if hasattr(dataset.domain, "value") else str(dataset.domain)
        if domain == "ai_incidents":
            return (
                "```\n"
                "McGregor, S. (2021) Preventing Repeated Real World AI "
                "Failures by Cataloging Incidents: The AI Incident Database. "
                "In Proceedings of the Thirty-Third Annual Conference on "
                "Innovative Applications of Artificial Intelligence (IAAI-21).\n"
                "```\n\n"
                f"**Snapshot accessed:** {schema.get('snapshot_date', 'Unknown')}\n"
                f"**Snapshot URL:** {schema.get('snapshot_url', 'Unknown')}\n"
            )
        return (
            f"Dataset: **{dataset.name}**\n"
            f"Accessed via DataSage on "
            f"{dataset.uploaded_at.strftime('%Y-%m-%d') if dataset.uploaded_at else 'Unknown'}\n"
        )

    def _assumptions_section(self, dataset: Dataset, eda_json: dict[str, Any]) -> str:
        del dataset, eda_json
        general = [
            "This README was auto-generated by DataSage AI agents â€” "
            "all findings should be reviewed by a domain expert before publication.",
            "Quality scores are computed automatically and may not reflect "
            "domain-specific data quality standards.",
            "Column role assignments (id, feature, target, etc.) are inferred "
            "heuristically and should be verified.",
        ]
        return "\n".join([f"- {line}" for line in general])

    def _get_source(self, dataset: Dataset) -> str:
        schema = dataset.schema_json if isinstance(dataset.schema_json, dict) else {}
        return str(schema.get("source", "Uploaded by user"))

    def _get_description(self, dataset: Dataset) -> str:
        if dataset.description:
            return dataset.description
        domain = dataset.domain.value if hasattr(dataset.domain, "value") else str(dataset.domain)
        if domain == "ai_incidents":
            return (
                "This dataset contains documented cases of AI system failures "
                "and harms, as cataloged by the AI Incident Database (AIID). "
                "Each record represents a distinct incident involving an AI system "
                "producing unintended or harmful outcomes."
            )
        return ""
